{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态价值函数:\n",
    "\n",
    "V(state) = 所有动作求和 -> 概率(action) * Q(state,action)\n",
    "\n",
    "对这个式子做变形得到:\n",
    "\n",
    "V(state) = 所有动作求和 -> 现概率(action) * \\[旧概率(action) / 现概率(action)\\] * Q(state,action)\n",
    "\n",
    "初始时可以认为现概率和旧概率相等,但随着模型的更新,现概率会变化.\n",
    "\n",
    "式子中的Q(state,action)可以用蒙特卡洛法估计.\n",
    "\n",
    "按照策略梯度的理论,状态价值取决于动作的质量,所以只要最大化V函数,就可以得到最好的动作策略."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "#定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        over = terminated or truncated\n",
    "\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n >= 200:\n",
    "            over = True\n",
    "\n",
    "        #没坚持到最后,扣分\n",
    "        if over and self.step_n < 200:\n",
    "            reward = -1000\n",
    "\n",
    "        return state, reward, over\n",
    "\n",
    "    #打印游戏图像\n",
    "    def show(self):\n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4699, 0.5301],\n",
       "         [0.4526, 0.5474]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[-0.1583],\n",
       "         [-0.1909]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#定义模型\n",
    "model_action = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 2),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "model_value = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_action(torch.randn(2, 4)), model_value(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GLT\\AppData\\Local\\Temp\\ipykernel_3140\\1112667714.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  state = torch.FloatTensor(state).reshape(-1, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-991.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        #根据概率采样\n",
    "        prob = model_action(torch.FloatTensor(s).reshape(1, 4))[0].tolist()\n",
    "        a = random.choices(range(2), weights=prob, k=1)[0]\n",
    "\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            display.clear_output(wait=True)\n",
    "            env.show()\n",
    "\n",
    "    state = torch.FloatTensor(state).reshape(-1, 4)\n",
    "    action = torch.LongTensor(action).reshape(-1, 1)\n",
    "    reward = torch.FloatTensor(reward).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor(next_state).reshape(-1, 4)\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_action = torch.optim.Adam(model_action.parameters(), lr=1e-3)\n",
    "optimizer_value = torch.optim.Adam(model_value.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "def requires_grad(model, value):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_value(state, reward, next_state, over):\n",
    "    requires_grad(model_action, False)\n",
    "    requires_grad(model_value, True)\n",
    "\n",
    "    #计算target\n",
    "    with torch.no_grad():\n",
    "        target = model_value(next_state)\n",
    "    target = target * 0.98 * (1 - over) + reward\n",
    "\n",
    "    #每批数据反复训练10次\n",
    "    for _ in range(10):\n",
    "        #计算value\n",
    "        value = model_value(state)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(value, target)\n",
    "        loss.backward()\n",
    "        optimizer_value.step()\n",
    "        optimizer_value.zero_grad()\n",
    "\n",
    "    #减去value相当于去基线\n",
    "    return (target - value).detach()\n",
    "\n",
    "\n",
    "value = train_value(state, reward, next_state, over)\n",
    "\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662.2925415039062"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_action(state, action, value):\n",
    "    requires_grad(model_action, True)\n",
    "    requires_grad(model_value, False)\n",
    "\n",
    "    #计算当前state的价值,其实就是Q(state,action),这里是用蒙特卡洛法估计的\n",
    "    delta = []\n",
    "    for i in range(len(value)):\n",
    "        s = 0\n",
    "        for j in range(i, len(value)):\n",
    "            s += value[j] * (0.98 * 0.95)**(j - i)\n",
    "        delta.append(s)\n",
    "    delta = torch.FloatTensor(delta).reshape(-1, 1)\n",
    "\n",
    "    #更新前的动作概率\n",
    "    with torch.no_grad():\n",
    "        prob_old = model_action(state).gather(dim=1, index=action)\n",
    "\n",
    "    #每批数据反复训练10次\n",
    "    for _ in range(10):\n",
    "        #更新后的动作概率\n",
    "        prob_new = model_action(state).gather(dim=1, index=action)\n",
    "\n",
    "        #求出概率的变化\n",
    "        ratio = prob_new / prob_old\n",
    "\n",
    "        #计算截断的和不截断的两份loss,取其中小的\n",
    "        surr1 = ratio * delta\n",
    "        surr2 = ratio.clamp(0.8, 1.2) * delta\n",
    "\n",
    "        loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        #更新参数\n",
    "        loss.backward()\n",
    "        optimizer_action.step()\n",
    "        optimizer_action.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "train_action(state, action, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 76.68888092041016 -976.35\n",
      "100 1.9910087585449219 149.8\n",
      "200 0.4413209557533264 200.0\n",
      "300 -0.5103653073310852 200.0\n",
      "400 0.21382583677768707 200.0\n",
      "500 0.2926938235759735 200.0\n",
      "600 1.0763553380966187 200.0\n",
      "700 -9.28822135925293 200.0\n",
      "800 1.0681346654891968 200.0\n",
      "900 -1.2053968906402588 200.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model_action.train()\n",
    "    model_value.train()\n",
    "\n",
    "    #训练N局\n",
    "    for epoch in range(1000):\n",
    "        #一个epoch最少玩N步\n",
    "        steps = 0\n",
    "        while steps < 200:\n",
    "            state, action, reward, next_state, over, _ = play()\n",
    "            steps += len(state)\n",
    "\n",
    "            #训练两个模型\n",
    "            delta = train_value(state, reward, next_state, over)\n",
    "            loss = train_action(state, action, delta)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第9章-策略梯度算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
